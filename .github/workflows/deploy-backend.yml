name: Deploy Backend to AWS EC2

on:
  push:
    branches: [ "main" ]
    paths:
      - 'backend/**'
      - 'docker-compose.prod.yml'
      - 'Caddyfile'
      - '.github/workflows/deploy-backend.yml'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}

      - name: Build and Push Backend Image
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          push: true
          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/vesco-backend:latest
          cache-from: type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME }}/vesco-backend:latest
          cache-to: type=inline

      - name: Ensure remote app directory exists
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USERNAME }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            set -e
            mkdir -p app

      - name: Remote diagnostics (before SCP)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USERNAME }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            set -e
            echo "whoami=$(whoami)"
            echo "pwd=$(pwd)"
            echo "HOME=$HOME"
            ls -ld app || true
            df -h || true
            command -v tar || true
            command -v gzip || true

      - name: Copy Compose and Caddy Files to EC2 (OpenSSH scp)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa

          # Add host key (avoid interactive prompt)
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

          # Ensure destination exists (defensive)
          ssh -i ~/.ssh/id_rsa "${{ secrets.EC2_USERNAME }}@${{ secrets.EC2_HOST }}" "mkdir -p app"

          # Copy files into ~/app
          scp -i ~/.ssh/id_rsa docker-compose.prod.yml Caddyfile "${{ secrets.EC2_USERNAME }}@${{ secrets.EC2_HOST }}:app/"

      - name: Deploy to EC2
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USERNAME }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            set -e

            # Create app directory if not exists
            mkdir -p app
            cd app

            echo "Disk before cleanup:";
            df -h || true

            # If disk is tight, clean up Docker artifacts (keep named volumes like db_data)
            docker compose -f docker-compose.prod.yml down || true
            docker container prune -f || true
            docker image prune -af || true
            docker builder prune -af || true

            echo "Disk after Docker cleanup:";
            df -h || true

            # Configure 2GB Swap Space to prevent OOM
            # First turn off any existing swap to resize
            # Only attempt swap setup if there's enough free disk and swap isn't already enabled.
            if ! swapon --show | grep -q '^/swapfile'; then
              avail_mb=$(df -Pm / | tail -1 | awk '{print $4}')
              if [ "$avail_mb" -ge 2500 ]; then
                sudo swapoff -a || true
                sudo rm -f /swapfile

                sudo fallocate -l 2G /swapfile
                sudo chmod 600 /swapfile
                sudo mkswap /swapfile
                sudo swapon /swapfile

                sudo sed -i '/swapfile/d' /etc/fstab
                echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
              else
                echo "Skipping swap creation (only ${avail_mb}MB free on /)"
              fi
            fi

            # Create .env file from GitHub Secrets
            # NOTE: Secrets sometimes accidentally include a trailing newline (copy/paste).
            # That breaks dotenv parsing and can make GOOGLE_CLIENT_ID look "missing" in the container.
            DOCKER_HUB_USERNAME_CLEAN=$(printf '%s' "${{ secrets.DOCKER_HUB_USERNAME }}" | tr -d '\r\n')
            DB_PASSWORD_CLEAN=$(printf '%s' "${{ secrets.DB_PASSWORD }}" | tr -d '\r\n')
            JWT_SECRET_CLEAN=$(printf '%s' "${{ secrets.JWT_SECRET }}" | tr -d '\r\n')
            GOOGLE_CLIENT_ID_CLEAN=$(printf '%s' "${{ secrets.GOOGLE_CLIENT_ID }}" | tr -d '\r\n')

            if [ -z "$DOCKER_HUB_USERNAME_CLEAN" ]; then echo "Missing DOCKER_HUB_USERNAME secret"; exit 1; fi
            if [ -z "$DB_PASSWORD_CLEAN" ]; then echo "Missing DB_PASSWORD secret"; exit 1; fi
            if [ -z "$JWT_SECRET_CLEAN" ]; then echo "Missing JWT_SECRET secret"; exit 1; fi
            if [ -z "$GOOGLE_CLIENT_ID_CLEAN" ]; then echo "Missing GOOGLE_CLIENT_ID secret"; exit 1; fi

            cat > .env <<EOF
            DATABASE_URL=postgresql://postgres:${DB_PASSWORD_CLEAN}@db:5432/vesco_db
            DB_PASSWORD=${DB_PASSWORD_CLEAN}
            JWT_SECRET=${JWT_SECRET_CLEAN}
            CLOUDINARY_CLOUD_NAME=${{ secrets.CLOUDINARY_CLOUD_NAME }}
            CLOUDINARY_API_KEY=${{ secrets.CLOUDINARY_API_KEY }}
            CLOUDINARY_API_SECRET=${{ secrets.CLOUDINARY_API_SECRET }}
            CORS_ORIGIN=${{ secrets.CORS_ORIGIN }}
            NODE_ENV=production
            PORT=5000
            DOCKER_HUB_USERNAME=${DOCKER_HUB_USERNAME_CLEAN}
            GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID_CLEAN}
            EOF

            # Confirm Google client id is present (print length only)
            echo "GOOGLE_CLIENT_ID length in .env:";
            awk -F= '/^GOOGLE_CLIENT_ID=/{print length($2)}' .env || true

            # Pull latest images
            docker compose -f docker-compose.prod.yml pull

            # Run migrations using the backend service definition
            # This ensures 'db' is healthy before running because of 'depends_on'
            # If a legacy migration previously failed, resolve it so deploy can continue.
            docker compose -f docker-compose.prod.yml run --rm backend npx prisma migrate resolve --rolled-back 20260124130807_add_auth_fields || true
            docker compose -f docker-compose.prod.yml run --rm backend npx prisma migrate resolve --rolled-back 20260129000000_add_linkedin_to_projects || true
            docker compose -f docker-compose.prod.yml run --rm backend npx prisma migrate deploy

            # Start the application
            docker compose -f docker-compose.prod.yml up -d --remove-orphans
